# adaptive-sparse-llm
Exploring cost-aware sparse Transformer architectures using Mixture-of-Experts (MoE).
